from fastapi import FastAPI, Body
from routes.codet5_routes import router as codet5_router
from routes.similaridade_routes import router as similaridade_router
from services.reindex_service import reindexar_todos, reindexar_blocos
from services.bloco_splitter_service import BlocoSplitterService
from services.embedding_service import EmbeddingRequest
from services.processar_embeddings import processar_embeddings
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch
import numpy as np
import faiss
import os
import logging

# Evita erro de duplica√ß√£o com bibliotecas que usam OpenMP
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("main")

# Cria√ß√£o da aplica√ß√£o FastAPI
app = FastAPI(title="Microservi√ßo de Corre√ß√£o com IA")

# üîÅ A√ß√µes ao iniciar a API
@app.on_event("startup")
async def startup_event():
    logger.info("üîÑ Reindexando todos os casos corrigidos ao iniciar a API...")
    reindexar_todos()
    processar_embeddings()
    logger.info("üß† Reindexando todos os blocos de c√≥digo corrigidos...")
    reindexar_blocos()

    logger.info("üîç Processando blocos de c√≥digo para treinamento e corre√ß√£o...")
    splitter = BlocoSplitterService()
    splitter.processar_todos()
    splitter.processar_novos()

    # Carregando modelo e tokenizer CodeT5
    try:
        model_dir = "./codet5p-220m-finetuned"
        model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)
        tokenizer = AutoTokenizer.from_pretrained(model_dir)


        # √çndice FAISS para embeddings + mapeamento dos c√≥digos adicionados
        index = faiss.IndexFlatL2(768)  # confere se o tamanho 768 est√° correto para seu modelo
        codigo_id_map = []

        # Armazenando tudo no app.state
        app.state.model = model
        app.state.tokenizer = tokenizer
        app.state.index = index
        app.state.codigo_id_map = codigo_id_map

        max_tokens = getattr(model.config, "n_positions", None)
        if max_tokens:
            logger.info(f"Modelo carregado com sucesso. Token m√°ximo suportado: {max_tokens}")
        else:
            logger.warning("N√£o foi poss√≠vel determinar o n√∫mero m√°ximo de tokens do modelo.")
    except Exception as e:
        logger.error("Erro ao carregar o modelo/tokenizer:")
        logger.exception(e)

# üîå Endpoint para verificar se a API est√° no ar
@app.get("/check")
def check_connection():
    return {"status": "OK", "message": "Python server is up and running!"}

# üîç Endpoint para adicionar c√≥digo ao √≠ndice FAISS
@app.post("/adicionar")
async def adicionar_codigo(req: EmbeddingRequest = Body(...)):
    tokenizer = app.state.tokenizer
    model = app.state.model
    index = app.state.index
    codigo_id_map = app.state.codigo_id_map

    entrada = f"{req.tipo}: {req.codigo}"
    tokens = tokenizer(entrada, return_tensors="pt", truncation=True, max_length=512)

    with torch.no_grad():
        outputs = model(**tokens)

    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
    index.add(np.array([embedding]))
    codigo_id_map.append(req.codigo)

    return {
        "message": "‚úÖ C√≥digo adicionado com sucesso!",
        "total_codigos": len(codigo_id_map)
    }

# üì¶ Inclus√£o das rotas
app.include_router(codet5_router, prefix="/codet5")
app.include_router(similaridade_router, prefix="/similaridade")
